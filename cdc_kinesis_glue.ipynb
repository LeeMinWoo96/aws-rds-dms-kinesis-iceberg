{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"# AWS Glue Studio Notebook\n",
				"##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"####  Run this cell to set up and start your interactive session.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Welcome to the Glue Interactive Sessions Kernel\n",
						"For more information on available magic commands, please type %help in any new cell.\n",
						"\n",
						"Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
						"Installed kernel version: 1.0.5 \n",
						"The following configurations have been updated: {'--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', '--datalake-formats': 'iceberg', '--JOB_NAME': 'cdc_kinesis_glue', '--TEMP_DIR': 's3://glue-cdc-temp/temp/', '--TempDir': 's3://glue-cdc-temp/temp/', '--enable-continuous-cloudwatch-log': 'true'}\n"
					]
				}
			],
			"source": [
				"%%configure \n",
				"{\n",
				"  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
				"  \"--datalake-formats\": \"iceberg\",\n",
				"  \"--JOB_NAME\": \"cdc_kinesis_glue\",\n",
				"  \"--TEMP_DIR\" : \"s3://glue-cdc-temp/temp/\",\n",
				"  \"--TempDir\" : \"s3://glue-cdc-temp/temp/\",\n",
				"  \"--enable-continuous-cloudwatch-log\" : \"true\"\n",
				"}"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Setting Glue version to: 3.0\n",
						"Previous number of workers: None\n",
						"Setting new number of workers to: 2\n",
						"Previous worker type: None\n",
						"Setting new worker type to: G.1X\n"
					]
				}
			],
			"source": [
				"%glue_version 3.0\n",
				"%number_of_workers 2\n",
				"%worker_type G.1X\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Trying to create a Glue session for the kernel.\n",
						"Session Type: glueetl\n",
						"Worker Type: G.1X\n",
						"Number of Workers: 2\n",
						"Session ID: 82c8cae1-d1fa-44b1-af80-01f0d67d2d31\n",
						"Applying the following default arguments:\n",
						"--glue_kernel_version 1.0.5\n",
						"--enable-glue-datacatalog true\n",
						"--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n",
						"--datalake-formats iceberg\n",
						"--JOB_NAME cdc_kinesis_glue\n",
						"--TEMP_DIR s3://glue-cdc-temp/temp/\n",
						"--TempDir s3://glue-cdc-temp/temp/\n",
						"--enable-continuous-cloudwatch-log true\n",
						"Waiting for session 82c8cae1-d1fa-44b1-af80-01f0d67d2d31 to get into ready status...\n",
						"Session 82c8cae1-d1fa-44b1-af80-01f0d67d2d31 has been created.\n",
						"\n"
					]
				}
			],
			"source": [
				"from pyspark.conf import SparkConf\n",
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"  \n",
				"from pyspark.sql.functions import col, desc, to_timestamp\n",
				"from pyspark.sql.window import Window\n",
				"from pyspark.sql.functions import row_number\n",
				"from pyspark.sql.functions import current_timestamp\n",
				"\n",
				"import logging\n",
				"\n",
				"# Set up logging\n",
				"logger = logging.getLogger()\n",
				"logger.setLevel(logging.DEBUG)\n",
				"    \n",
				"conf = SparkConf()\n",
				"ICEBERG_S3_PATH = {}\n",
				"CATALOG = \"glue_catalog\"\n",
				"DATABASE = {}\n",
				"TABLE_NAME = {}\n",
				"WINDOW_SIZE = \"60 seconds\"\n",
				"PRIMARY_KEY = {}"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"def setSparkIcebergConf() -> SparkConf:\n",
				"    conf_list = [\n",
				"        (\"spark.sql.defaultCatalog\", \"glue_catalog\"),\n",
				"        (f\"spark.sql.catalog.{CATALOG}\", \"org.apache.iceberg.spark.SparkCatalog\"),\n",
				"        (f\"spark.sql.catalog.{CATALOG}.warehouse\", ICEBERG_S3_PATH),\n",
				"        (f\"spark.sql.catalog.{CATALOG}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\"),\n",
				"        (f\"spark.sql.catalog.{CATALOG}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\"),\n",
				"        (f\"spark.sql.catalog.{CATALOG}.lock-impl\", \"org.apache.iceberg.aws.glue.DynamoLockManager\"),\n",
				"        (f\"spark.sql.catalog.{CATALOG}.lock.table\", f\"{CATALOG}_lock\"),\n",
				"        (\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"),\n",
				"        (\"spark.sql.iceberg.handle-timestamp-without-timezone\",\"true\")\n",
				"    ]\n",
				"    spark_conf = SparkConf().setAll(conf_list)\n",
				"    \n",
				"    return spark_conf\n",
				"    "
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"\n",
				"conf = setSparkIcebergConf()\n",
				"sc = SparkContext.getOrCreate(conf=conf)\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)\n",
				"args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
				"job.init(args['JOB_NAME'], args)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"from awsglue.dynamicframe import DynamicFrame\n",
				"from pyspark.sql.functions import col, lit, row_number, current_timestamp, desc\n",
				"from pyspark.sql import Window\n",
				"\n",
				"def processBatch(data_frame, batch_id):\n",
				"    print(\"start\")\n",
				"    logger.info(\"start\")\n",
				"    try:\n",
				"        # Step 1: Convert DynamicFrame to DataFrame and initialize Iceberg DataFrame\n",
				"        try:\n",
				"            stream_data_dynf = DynamicFrame.fromDF(data_frame, glueContext, \"from_data_frame\")  # Kinesis\n",
				"            _df = spark.sql(f\"select * from {DATABASE}.{TABLE_NAME} LIMIT 0\")  # Iceberg \n",
				"            print(\"step 1: Initialized DynamicFrame and Iceberg DataFrame\")\n",
				"            logger.info(\"step 1: Initialized DynamicFrame and Iceberg DataFrame\")\n",
				"        except Exception as e:\n",
				"            print(f\"Error in step 1: {str(e)}\")\n",
				"            logger.error(f\"Error in step 1: {str(e)}\")\n",
				"            return\n",
				"\n",
				"        # Log schema and columns of _df (Iceberg DataFrame)\n",
				"        try:\n",
				"            print(f\"Columns in _df: {_df.columns}\")\n",
				"            # logger.info(f\"Columns in _df: {_df.columns}\")\n",
				"            _df_schema_str = _df._jdf.schema().treeString()\n",
				"            # print(f\"Schema of _df:\\n{_df_schema_str}\")\n",
				"            # logger.info(f\"Schema of _df:\\n{_df_schema_str}\")\n",
				"        except Exception as e:\n",
				"            print(f\"Error while logging Iceberg DataFrame schema: {str(e)}\")\n",
				"            logger.error(f\"Error while logging Iceberg DataFrame schema: {str(e)}\")\n",
				"        \n",
				"        # Step 2: Convert to DataFrame and select columns\n",
				"        try:\n",
				"            stream_data_df = stream_data_dynf.toDF()\n",
				"            if \"data\" not in stream_data_df.columns or stream_data_df.filter(col(\"data\").isNull()).count() > 0:\n",
				"                print(\"Detected records with only metadata and no data. Skipping these records.\")\n",
				"                logger.warning(\"Detected records with only metadata and no data. Skipping these records.\")\n",
				"                return  # Skip processing this batch\n",
				"            \n",
				"            \n",
				"            # seq 컬럼이 null인 레코드는 제외\n",
				"            stream_data_df = stream_data_df.filter(col(\"data.seq\").isNotNull())\n",
				"\n",
				"            print(f\"Sample data in stream_data_df:\\n{stream_data_df.show(5, truncate=False)}\")\n",
				"            logger.info(f\"Sample data in stream_data_df:\\n{stream_data_df.show(5, truncate=False)}\")\n",
				"            print(f\"Columns in stream_data_df: {stream_data_df.columns}\")\n",
				"            # logger.info(f\"Columns in stream_data_df: {stream_data_df.columns}\")\n",
				"            stream_data_df_schema_str = stream_data_df._jdf.schema().treeString()\n",
				"            print(f\"Schema of stream_data_df:\\n{stream_data_df_schema_str}\")\n",
				"            logger.info(f\"Schema of stream_data_df:\\n{stream_data_df_schema_str}\")\n",
				"        except Exception as e:\n",
				"            print(f\"Error in step 2: {str(e)}\")\n",
				"            logger.error(f\"Error in step 2: {str(e)}\")\n",
				"            return\n",
				"\n",
				"        try:\n",
				"            cdc_df = stream_data_df.select(\n",
				"                col('data.*'), \n",
				"                col('metadata.operation').alias('_op'),\n",
				"                col('metadata.timestamp').alias('_op_timestamp')\n",
				"            )\n",
				"            # print(f\"step 2: Columns in cdc_df after selection: {cdc_df.columns}\")\n",
				"            # logger.info(f\"step 2: Columns in cdc_df after selection: {cdc_df.columns}\")\n",
				"            print(f\"Sample data in cdc_df:\\n{cdc_df.show(5, truncate=False)}\")\n",
				"            logger.info(f\"Sample data in cdc_df:\\n{cdc_df.show(5, truncate=False)}\")\n",
				"        except Exception as e:\n",
				"            print(f\"Error in selecting columns in step 2: {str(e)}\")\n",
				"            logger.error(f\"Error in selecting columns in step 2: {str(e)}\")\n",
				"            return\n",
				"\n",
				"        # Step 3: Ensure the cdc_df schema matches the Iceberg table schema\n",
				"        try:\n",
				"    # Iceberg 테이블 컬럼과 CDC DataFrame 컬럼을 모두 소문자로 변환하여 비교\n",
				"            iceberg_columns = set([col.lower() for col in _df.schema.names])\n",
				"            cdc_columns = set([col.lower() for col in cdc_df.columns])\n",
				"\n",
				"            missing_columns = iceberg_columns - cdc_columns\n",
				"            if missing_columns:\n",
				"                print(f\"Schema mismatch: missing columns in stream data - {missing_columns}\")\n",
				"                logger.warning(f\"Schema mismatch: missing columns in stream data - {missing_columns}\")\n",
				"\n",
				"                # 새 컬럼에 대해서만 null 값 추가\n",
				"                for col_name in missing_columns:\n",
				"                    if col_name not in cdc_columns:\n",
				"                        # 소문자로 변환된 컬럼을 기반으로 데이터 타입 추가\n",
				"                        original_col_name = next((name for name in _df.schema.names if name.lower() == col_name), None)\n",
				"                        if original_col_name:\n",
				"                            cdc_df = cdc_df.withColumn(col_name, lit(None).cast(_df.schema[original_col_name].dataType))\n",
				"\n",
				"                print(f\"step 3: Added missing columns: {missing_columns}\")\n",
				"                logger.info(f\"step 3: Added missing columns: {missing_columns}\")\n",
				"\n",
				"            print(f\"Sample data in cdc_df after adding missing columns:\\n{cdc_df.show(5, truncate=False)}\")\n",
				"            logger.info(f\"Sample data in cdc_df after adding missing columns:\\n{cdc_df.show(5, truncate=False)}\")\n",
				"        except Exception as e:\n",
				"            print(f\"Error in step 3: {str(e)}\")\n",
				"            logger.error(f\"Error in step 3: {str(e)}\")\n",
				"            return\n",
				"\n",
				"\n",
				"        # Step 4: Apply window function and deduplication\n",
				"        try:\n",
				"            print(\"step 4: Applying window function and deduplication\")\n",
				"            logger.info(\"step 4: Applying window function and deduplication\")\n",
				"            window = Window.partitionBy(PRIMARY_KEY).orderBy(desc(\"_op_timestamp\"))\n",
				"            deduped_cdc_df = cdc_df.withColumn(\"_row\", row_number().over(window))\\\n",
				"                                   .filter(col(\"_row\") == 1).drop(\"_row\")\\\n",
				"                                   .select([col(c) for c in _df.schema.names])\n",
				"            print(f\"step 4: Columns in deduped_cdc_df after deduplication: {deduped_cdc_df.columns}\")\n",
				"            logger.info(f\"step 4: Columns in deduped_cdc_df after deduplication: {deduped_cdc_df.columns}\")\n",
				"            print(f\"Sample data in deduped_cdc_df:\\n{deduped_cdc_df.show(5, truncate=False)}\")\n",
				"            logger.info(f\"Sample data in deduped_cdc_df:\\n{deduped_cdc_df.show(5, truncate=False)}\")\n",
				"        except Exception as e:\n",
				"            print(f\"Error in step 4: {str(e)}\")\n",
				"            logger.error(f\"Error in step 4: {str(e)}\")\n",
				"            return\n",
				"\n",
				"        # Step 5: Add 'trans_time' column\n",
				"        try:\n",
				"            deduped_cdc_df = deduped_cdc_df.withColumn('trans_time', current_timestamp())\n",
				"            print(\"step 5: Added 'trans_time' column to deduped_cdc_df\")\n",
				"            logger.info(\"step 5: Added 'trans_time' column to deduped_cdc_df\")\n",
				"            print(f\"Sample data in deduped_cdc_df with 'trans_time':\\n{deduped_cdc_df.show(5, truncate=False)}\")\n",
				"            logger.info(f\"Sample data in deduped_cdc_df with 'trans_time':\\n{deduped_cdc_df.show(5, truncate=False)}\")\n",
				"        except Exception as e:\n",
				"            print(f\"Error in step 5: {str(e)}\")\n",
				"            logger.error(f\"Error in step 5: {str(e)}\")\n",
				"            return\n",
				"\n",
				"        # Step 6: Upsert operation\n",
				"        try:\n",
				"            upserted_df = deduped_cdc_df.filter(col('_op') != 'delete')\n",
				"            upserted_count = upserted_df.count()\n",
				"            print(f\"step 6: Upserted rows count: {upserted_count}\")\n",
				"            logger.info(f\"step 6: Upserted rows count: {upserted_count}\")\n",
				"            if upserted_count > 0:\n",
				"                print(\"Sample data in upserted_df:\")\n",
				"                upserted_df.show(5, truncate=False)\n",
				"\n",
				"                print(\"Schema of upserted_df:\")\n",
				"                upserted_df.printSchema()\n",
				"                upserted_df.createOrReplaceTempView(f\"{TABLE_NAME}_upsert\")\n",
				"                print(f\"step 6: Created temporary view for upsert: {TABLE_NAME}_upsert\")\n",
				"                logger.info(f\"step 6: Created temporary view for upsert: {TABLE_NAME}_upsert\")\n",
				"                spark.sql(f\"\"\"\n",
				"                    MERGE INTO {DATABASE}.{TABLE_NAME} t \n",
				"                    USING {TABLE_NAME}_upsert s \n",
				"                    ON t.{PRIMARY_KEY} = s.{PRIMARY_KEY} \n",
				"                    WHEN MATCHED THEN UPDATE SET * \n",
				"                    WHEN NOT MATCHED THEN INSERT *\n",
				"                \"\"\")\n",
				"                print(\"step 6: Completed MERGE operation\")\n",
				"                logger.info(\"step 6: Completed MERGE operation\")\n",
				"        except Exception as e:\n",
				"            print(f\"Error in step 6 (Upsert operation): {str(e)}\")\n",
				"            logger.error(f\"Error in step 6 (Upsert operation): {str(e)}\")\n",
				"            return\n",
				"\n",
				"        # Step 7: Delete operation\n",
				"        try:\n",
				"            deleted_df = deduped_cdc_df.filter(col('_op') == 'delete')\n",
				"            deleted_count = deleted_df.count()\n",
				"            print(f\"step 7: Deleted rows count: {deleted_count}\")\n",
				"            logger.info(f\"step 7: Deleted rows count: {deleted_count}\")\n",
				"            if deleted_count > 0:\n",
				"                deleted_df.createOrReplaceTempView(f\"{TABLE_NAME}_delete\")\n",
				"                print(f\"step 7: Created temporary view for delete: {TABLE_NAME}_delete\")\n",
				"                logger.info(f\"step 7: Created temporary view for delete: {TABLE_NAME}_delete\")\n",
				"                spark.sql(f\"DELETE FROM {DATABASE}.{TABLE_NAME} t USING {TABLE_NAME}_delete s WHERE t.{PRIMARY_KEY} = s.{PRIMARY_KEY}\")\n",
				"                print(\"step 7: Completed DELETE operation\")\n",
				"                logger.info(\"step 7: Completed DELETE operation\")\n",
				"        except Exception as e:\n",
				"            print(f\"Error in step 7 (Delete operation): {str(e)}\")\n",
				"            logger.error(f\"Error in step 7 (Delete operation): {str(e)}\")\n",
				"            return\n",
				"\n",
				"    except Exception as e:\n",
				"        print(f\"Error in processBatch: {str(e)}\")\n",
				"        logger.error(f\"Error in processBatch: {str(e)}\")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"NameError: name 'glueContext' is not defined\n"
					]
				}
			],
			"source": [
				"from pyspark.sql import functions as F\n",
				"kds_df = glueContext.create_data_frame.from_options(\n",
				"    connection_type=\"kinesis\",\n",
				"    connection_options={\n",
				"        \"typeOfData\": \"kinesis\",\n",
				"        \"streamARN\": \"{}\",\n",
				"        \"classification\": \"json\",\n",
				"        \"startingPosition\": \"LATEST\",\n",
				"        \"inferSchema\": \"true\",\n",
				"    },\n",
				"    transformation_ctx=\"kds_df\",\n",
				"    additional_options={\"samplingRatio\": 0.15}\n",
				")\n",
				"\n",
				"# 변환된 스키마 확인\n",
				"checkpointPath = f\"{args['TempDir']}/{args['JOB_NAME']}/checkpoint/\"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"glueContext.forEachBatch(frame=kds_df, batch_function = processBatch,\n",
				"                         options={\"windowSize\":WINDOW_SIZE, \"checkpointLocation\":checkpointPath})"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 19,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"start\n",
						"Error in step 1: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table mst_item_iceberg. StorageDescriptor#InputFormat cannot be null for table: mst_item_iceberg (Service: null; Status Code: 0; Error Code: null; Request ID: null; Proxy: null)\n"
					]
				}
			],
			"source": [
				"processBatch(collected_df,1)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"GlueArgumentError: the following arguments are required: --JOB_NAME\n"
					]
				}
			],
			"source": [
				"job.commit()"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		},
		"toc-autonumbering": false,
		"toc-showcode": false,
		"toc-showtags": true
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
