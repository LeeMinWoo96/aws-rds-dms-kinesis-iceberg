{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		},
		"toc-showcode": false,
		"toc-showtags": true,
		"toc-autonumbering": false
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%%configure \n{\n  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n  \"--datalake-formats\": \"iceberg\",\n  \"--JOB_NAME\": \"cdc_kinesis_glue\",\n  \"--TEMP_DIR\" : \"s3://glue-cdc-temp/temp/\",\n  \"--TempDir\" : \"s3://glue-cdc-temp/temp/\",\n  \"--enable-continuous-cloudwatch-log\" : \"true\"\n}",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \nThe following configurations have been updated: {'--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', '--datalake-formats': 'iceberg', '--JOB_NAME': 'cdc_kinesis_glue', '--TEMP_DIR': 's3://glue-cdc-temp/temp/', '--TempDir': 's3://glue-cdc-temp/temp/', '--enable-continuous-cloudwatch-log': 'true'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%glue_version 3.0\n%number_of_workers 2\n%worker_type G.1X\n",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "Setting Glue version to: 3.0\nPrevious number of workers: None\nSetting new number of workers to: 2\nPrevious worker type: None\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.conf import SparkConf\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nfrom pyspark.sql.functions import col, desc, to_timestamp\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\nfrom pyspark.sql.functions import current_timestamp\n\nimport logging\n\n# Set up logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\n    \nconf = SparkConf()\nICEBERG_S3_PATH = \"s3://aiscanner-iceberg/MST_ITEM/\"\nCATALOG = \"glue_catalog\"\nDATABASE = \"orapp_iceberg\"\nTABLE_NAME = \"mst_item_iceberg\"\nWINDOW_SIZE = \"60 seconds\"\nPRIMARY_KEY = \"seq\"",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 2\nSession ID: 82c8cae1-d1fa-44b1-af80-01f0d67d2d31\nApplying the following default arguments:\n--glue_kernel_version 1.0.5\n--enable-glue-datacatalog true\n--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n--datalake-formats iceberg\n--JOB_NAME cdc_kinesis_glue\n--TEMP_DIR s3://glue-cdc-temp/temp/\n--TempDir s3://glue-cdc-temp/temp/\n--enable-continuous-cloudwatch-log true\nWaiting for session 82c8cae1-d1fa-44b1-af80-01f0d67d2d31 to get into ready status...\nSession 82c8cae1-d1fa-44b1-af80-01f0d67d2d31 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def setSparkIcebergConf() -> SparkConf:\n    conf_list = [\n        (\"spark.sql.defaultCatalog\", \"glue_catalog\"),\n        (f\"spark.sql.catalog.{CATALOG}\", \"org.apache.iceberg.spark.SparkCatalog\"),\n        (f\"spark.sql.catalog.{CATALOG}.warehouse\", ICEBERG_S3_PATH),\n        (f\"spark.sql.catalog.{CATALOG}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\"),\n        (f\"spark.sql.catalog.{CATALOG}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\"),\n        (f\"spark.sql.catalog.{CATALOG}.lock-impl\", \"org.apache.iceberg.aws.glue.DynamoLockManager\"),\n        (f\"spark.sql.catalog.{CATALOG}.lock.table\", f\"{CATALOG}_lock\"),\n        (\"spark.sql.extensions\",\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"),\n        (\"spark.sql.iceberg.handle-timestamp-without-timezone\",\"true\")\n    ]\n    spark_conf = SparkConf().setAll(conf_list)\n    \n    return spark_conf\n    ",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\nconf = setSparkIcebergConf()\nsc = SparkContext.getOrCreate(conf=conf)\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\njob.init(args['JOB_NAME'], args)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from awsglue.dynamicframe import DynamicFrame\nfrom pyspark.sql.functions import col, lit, row_number, current_timestamp, desc\nfrom pyspark.sql import Window\n\ndef processBatch(data_frame, batch_id):\n    print(\"start\")\n    logger.info(\"start\")\n    try:\n        # Step 1: Convert DynamicFrame to DataFrame and initialize Iceberg DataFrame\n        try:\n            stream_data_dynf = DynamicFrame.fromDF(data_frame, glueContext, \"from_data_frame\")  # Kinesis\n            _df = spark.sql(f\"select * from {DATABASE}.{TABLE_NAME} LIMIT 0\")  # Iceberg \n            print(\"step 1: Initialized DynamicFrame and Iceberg DataFrame\")\n            logger.info(\"step 1: Initialized DynamicFrame and Iceberg DataFrame\")\n        except Exception as e:\n            print(f\"Error in step 1: {str(e)}\")\n            logger.error(f\"Error in step 1: {str(e)}\")\n            return\n\n        # Log schema and columns of _df (Iceberg DataFrame)\n        try:\n            print(f\"Columns in _df: {_df.columns}\")\n            # logger.info(f\"Columns in _df: {_df.columns}\")\n            _df_schema_str = _df._jdf.schema().treeString()\n            # print(f\"Schema of _df:\\n{_df_schema_str}\")\n            # logger.info(f\"Schema of _df:\\n{_df_schema_str}\")\n        except Exception as e:\n            print(f\"Error while logging Iceberg DataFrame schema: {str(e)}\")\n            logger.error(f\"Error while logging Iceberg DataFrame schema: {str(e)}\")\n        \n        # Step 2: Convert to DataFrame and select columns\n        try:\n            stream_data_df = stream_data_dynf.toDF()\n            if \"data\" not in stream_data_df.columns or stream_data_df.filter(col(\"data\").isNull()).count() > 0:\n                print(\"Detected records with only metadata and no data. Skipping these records.\")\n                logger.warning(\"Detected records with only metadata and no data. Skipping these records.\")\n                return  # Skip processing this batch\n            \n            \n            # seq 컬럼이 null인 레코드는 제외\n            stream_data_df = stream_data_df.filter(col(\"data.seq\").isNotNull())\n\n            print(f\"Sample data in stream_data_df:\\n{stream_data_df.show(5, truncate=False)}\")\n            logger.info(f\"Sample data in stream_data_df:\\n{stream_data_df.show(5, truncate=False)}\")\n            print(f\"Columns in stream_data_df: {stream_data_df.columns}\")\n            # logger.info(f\"Columns in stream_data_df: {stream_data_df.columns}\")\n            stream_data_df_schema_str = stream_data_df._jdf.schema().treeString()\n            print(f\"Schema of stream_data_df:\\n{stream_data_df_schema_str}\")\n            logger.info(f\"Schema of stream_data_df:\\n{stream_data_df_schema_str}\")\n        except Exception as e:\n            print(f\"Error in step 2: {str(e)}\")\n            logger.error(f\"Error in step 2: {str(e)}\")\n            return\n\n        try:\n            cdc_df = stream_data_df.select(\n                col('data.*'), \n                col('metadata.operation').alias('_op'),\n                col('metadata.timestamp').alias('_op_timestamp')\n            )\n            # print(f\"step 2: Columns in cdc_df after selection: {cdc_df.columns}\")\n            # logger.info(f\"step 2: Columns in cdc_df after selection: {cdc_df.columns}\")\n            print(f\"Sample data in cdc_df:\\n{cdc_df.show(5, truncate=False)}\")\n            logger.info(f\"Sample data in cdc_df:\\n{cdc_df.show(5, truncate=False)}\")\n        except Exception as e:\n            print(f\"Error in selecting columns in step 2: {str(e)}\")\n            logger.error(f\"Error in selecting columns in step 2: {str(e)}\")\n            return\n\n        # Step 3: Ensure the cdc_df schema matches the Iceberg table schema\n        try:\n    # Iceberg 테이블 컬럼과 CDC DataFrame 컬럼을 모두 소문자로 변환하여 비교\n            iceberg_columns = set([col.lower() for col in _df.schema.names])\n            cdc_columns = set([col.lower() for col in cdc_df.columns])\n\n            missing_columns = iceberg_columns - cdc_columns\n            if missing_columns:\n                print(f\"Schema mismatch: missing columns in stream data - {missing_columns}\")\n                logger.warning(f\"Schema mismatch: missing columns in stream data - {missing_columns}\")\n\n                # 새 컬럼에 대해서만 null 값 추가\n                for col_name in missing_columns:\n                    if col_name not in cdc_columns:\n                        # 소문자로 변환된 컬럼을 기반으로 데이터 타입 추가\n                        original_col_name = next((name for name in _df.schema.names if name.lower() == col_name), None)\n                        if original_col_name:\n                            cdc_df = cdc_df.withColumn(col_name, lit(None).cast(_df.schema[original_col_name].dataType))\n\n                print(f\"step 3: Added missing columns: {missing_columns}\")\n                logger.info(f\"step 3: Added missing columns: {missing_columns}\")\n\n            print(f\"Sample data in cdc_df after adding missing columns:\\n{cdc_df.show(5, truncate=False)}\")\n            logger.info(f\"Sample data in cdc_df after adding missing columns:\\n{cdc_df.show(5, truncate=False)}\")\n        except Exception as e:\n            print(f\"Error in step 3: {str(e)}\")\n            logger.error(f\"Error in step 3: {str(e)}\")\n            return\n\n\n        # Step 4: Apply window function and deduplication\n        try:\n            print(\"step 4: Applying window function and deduplication\")\n            logger.info(\"step 4: Applying window function and deduplication\")\n            window = Window.partitionBy(PRIMARY_KEY).orderBy(desc(\"_op_timestamp\"))\n            deduped_cdc_df = cdc_df.withColumn(\"_row\", row_number().over(window))\\\n                                   .filter(col(\"_row\") == 1).drop(\"_row\")\\\n                                   .select([col(c) for c in _df.schema.names])\n            print(f\"step 4: Columns in deduped_cdc_df after deduplication: {deduped_cdc_df.columns}\")\n            logger.info(f\"step 4: Columns in deduped_cdc_df after deduplication: {deduped_cdc_df.columns}\")\n            print(f\"Sample data in deduped_cdc_df:\\n{deduped_cdc_df.show(5, truncate=False)}\")\n            logger.info(f\"Sample data in deduped_cdc_df:\\n{deduped_cdc_df.show(5, truncate=False)}\")\n        except Exception as e:\n            print(f\"Error in step 4: {str(e)}\")\n            logger.error(f\"Error in step 4: {str(e)}\")\n            return\n\n        # Step 5: Add 'trans_time' column\n        try:\n            deduped_cdc_df = deduped_cdc_df.withColumn('trans_time', current_timestamp())\n            print(\"step 5: Added 'trans_time' column to deduped_cdc_df\")\n            logger.info(\"step 5: Added 'trans_time' column to deduped_cdc_df\")\n            print(f\"Sample data in deduped_cdc_df with 'trans_time':\\n{deduped_cdc_df.show(5, truncate=False)}\")\n            logger.info(f\"Sample data in deduped_cdc_df with 'trans_time':\\n{deduped_cdc_df.show(5, truncate=False)}\")\n        except Exception as e:\n            print(f\"Error in step 5: {str(e)}\")\n            logger.error(f\"Error in step 5: {str(e)}\")\n            return\n\n        # Step 6: Upsert operation\n        try:\n            upserted_df = deduped_cdc_df.filter(col('_op') != 'delete')\n            upserted_count = upserted_df.count()\n            print(f\"step 6: Upserted rows count: {upserted_count}\")\n            logger.info(f\"step 6: Upserted rows count: {upserted_count}\")\n            if upserted_count > 0:\n                print(\"Sample data in upserted_df:\")\n                upserted_df.show(5, truncate=False)\n\n                print(\"Schema of upserted_df:\")\n                upserted_df.printSchema()\n                upserted_df.createOrReplaceTempView(f\"{TABLE_NAME}_upsert\")\n                print(f\"step 6: Created temporary view for upsert: {TABLE_NAME}_upsert\")\n                logger.info(f\"step 6: Created temporary view for upsert: {TABLE_NAME}_upsert\")\n                spark.sql(f\"\"\"\n                    MERGE INTO {DATABASE}.{TABLE_NAME} t \n                    USING {TABLE_NAME}_upsert s \n                    ON t.{PRIMARY_KEY} = s.{PRIMARY_KEY} \n                    WHEN MATCHED THEN UPDATE SET * \n                    WHEN NOT MATCHED THEN INSERT *\n                \"\"\")\n                print(\"step 6: Completed MERGE operation\")\n                logger.info(\"step 6: Completed MERGE operation\")\n        except Exception as e:\n            print(f\"Error in step 6 (Upsert operation): {str(e)}\")\n            logger.error(f\"Error in step 6 (Upsert operation): {str(e)}\")\n            return\n\n        # Step 7: Delete operation\n        try:\n            deleted_df = deduped_cdc_df.filter(col('_op') == 'delete')\n            deleted_count = deleted_df.count()\n            print(f\"step 7: Deleted rows count: {deleted_count}\")\n            logger.info(f\"step 7: Deleted rows count: {deleted_count}\")\n            if deleted_count > 0:\n                deleted_df.createOrReplaceTempView(f\"{TABLE_NAME}_delete\")\n                print(f\"step 7: Created temporary view for delete: {TABLE_NAME}_delete\")\n                logger.info(f\"step 7: Created temporary view for delete: {TABLE_NAME}_delete\")\n                spark.sql(f\"DELETE FROM {DATABASE}.{TABLE_NAME} t USING {TABLE_NAME}_delete s WHERE t.{PRIMARY_KEY} = s.{PRIMARY_KEY}\")\n                print(\"step 7: Completed DELETE operation\")\n                logger.info(\"step 7: Completed DELETE operation\")\n        except Exception as e:\n            print(f\"Error in step 7 (Delete operation): {str(e)}\")\n            logger.error(f\"Error in step 7 (Delete operation): {str(e)}\")\n            return\n\n    except Exception as e:\n        print(f\"Error in processBatch: {str(e)}\")\n        logger.error(f\"Error in processBatch: {str(e)}\")\n",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import functions as F\nkds_df = glueContext.create_data_frame.from_options(\n    connection_type=\"kinesis\",\n    connection_options={\n        \"typeOfData\": \"kinesis\",\n        \"streamARN\": \"arn:aws:kinesis:ap-northeast-2:646298132551:stream/cdc-test\",\n        \"classification\": \"json\",\n        \"startingPosition\": \"LATEST\",\n        \"inferSchema\": \"true\",\n    },\n    transformation_ctx=\"kds_df\",\n    additional_options={\"samplingRatio\": 0.15}\n)\n\n# 변환된 스키마 확인\ncheckpointPath = f\"{args['TempDir']}/{args['JOB_NAME']}/checkpoint/\"\n",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "NameError: name 'glueContext' is not defined\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "glueContext.forEachBatch(frame=kds_df, batch_function = processBatch,\n                         options={\"windowSize\":WINDOW_SIZE, \"checkpointLocation\":checkpointPath})",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# spark.sql(f\"\"\"\n#                     MERGE INTO {DATABASE}.{TABLE_NAME} t \n#                     USING {TABLE_NAME}_upsert s \n#                     ON t.{PRIMARY_KEY} = s.{PRIMARY_KEY} \n#                     WHEN MATCHED THEN UPDATE SET * \n#                     WHEN NOT MATCHED THEN INSERT *\n#                 \"\"\")",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, DoubleType\n# schema = StructType([\n#     StructField(\"data\", StructType([\n#         StructField(\"SEQ\", IntegerType(), True),\n#         StructField(\"CMP_CD\", StringType(), True),\n#         StructField(\"BRAND_CD\", StringType(), True),\n#         StructField(\"STORE_TP\", IntegerType(), True),\n#         StructField(\"ITEM_CD\", StringType(), True),\n#         StructField(\"ITEM_SEQ\", IntegerType(), True),\n#         StructField(\"AIRPORT_DIV\", IntegerType(), True),\n#         StructField(\"ITEM_NM\", StringType(), True),\n#         StructField(\"IMAGE_PATH\", StringType(), True),\n#         StructField(\"DETAIL_COUNT\", IntegerType(), True),\n#         StructField(\"ITEM_BARCODE\", StringType(), True),\n#         StructField(\"ETC\", StringType(), True),\n#         StructField(\"USE_YN\", StringType(), True),\n#         StructField(\"INST_USER\", StringType(), True),\n#         StructField(\"UPD_USER\", StringType(), True),\n#         StructField(\"UPD_TIME\", StringType(), True),\n#         StructField(\"TRAIN_CD\", StringType(), True)\n#     ]), True),\n#     StructField(\"metadata\", StructType([\n#         StructField(\"timestamp\", StringType(), True),\n#         StructField(\"record-type\", StringType(), True),\n#         StructField(\"operation\", StringType(), True),\n#         StructField(\"partition-key-type\", StringType(), True),\n#         StructField(\"schema-name\", StringType(), True),\n#         StructField(\"table-name\", StringType(), True)\n#     ]), True)\n# ])\n\n# # 2. Kinesis에서 스트리밍 데이터 읽기\n# kds_df = glueContext.create_data_frame.from_options(\n#     connection_type=\"kinesis\",\n#     connection_options={\n#         \"typeOfData\": \"kinesis\",\n#         \"streamARN\": \"arn:aws:kinesis:ap-northeast-2:646298132551:stream/cdc-test\",\n#         \"classification\": \"json\",\n#         \"startingPosition\": \"TRIM_HORIZON\",\n#         \"inferSchema\": \"false\",\n#     }\n# )\n",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# # inferSchema를 true로 설정하고 데이터를 자동으로 추론\n# kds_df = glueContext.create_data_frame.from_options(\n#     connection_type=\"kinesis\",\n#     connection_options={\n#         \"typeOfData\": \"kinesis\",\n#         \"streamARN\": \"arn:aws:kinesis:ap-northeast-2:646298132551:stream/cdc-test\",\n#         \"classification\": \"json\",\n#         \"startingPosition\": \"TRIM_HORIZON\",\n#         \"inferSchema\": \"true\",  # 스키마 추론을 활성화\n#     },\n# )\n\n# # 샘플 데이터를 확인하여 스키마가 어떻게 되어있는지 확인",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# from pyspark.sql import DataFrame\n\n# # 스트리밍을 통해 10건만 수집하는 함수\n# def collect_stream_data(stream_df, max_records=10):\n#     collected_data = []\n    \n#     query = stream_df.writeStream \\\n#         .outputMode(\"append\") \\\n#         .format(\"memory\") \\\n#         .queryName(\"stream_data\") \\\n#         .trigger(processingTime=\"1 second\") \\\n#         .start()\n\n#     # 데이터가 10건 이상 수집되면 중지\n#     while len(collected_data) < max_records:\n#         # 메모리 테이블에서 데이터 가져오기\n#         stream_data = spark.sql(\"SELECT * FROM stream_data\")\n#         current_count = stream_data.count()\n\n#         # 새로운 데이터를 수집\n#         if current_count > 0:\n#             collected_data.extend(stream_data.collect())\n        \n#         # 수집된 데이터가 max_records에 도달하면 종료\n#         if len(collected_data) >= max_records:\n#             query.stop()\n#             break\n    \n#     # 수집된 데이터를 DataFrame으로 변환하여 반환\n#     collected_df = spark.createDataFrame(collected_data[:max_records], schema=stream_df.schema)\n#     return collected_df\n\n# # 수집된 데이터를 DataFrame으로 저장\n# collected_df = collect_stream_data(kds_df, max_records=10)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# import json\n# from pyspark.sql.functions import lit\n# from pyspark.sql.types import StringType\n\n# # Iceberg 테이블의 컬럼 목록\n# iceberg_columns = set({\n#     \"SEQ\", \"CMP_CD\", \"BRAND_CD\", \"STORE_TP\", \"ITEM_CD\", \"ITEM_SEQ\", \"SPCN_ITEM_CD\", \n#     \"SPCN_ITEM_SEQ\", \"AIRPORT_DIV\", \"ITEM_NM\", \"IMAGE_THUM_PATH\", \"IMAGE_PATH\", \n#     \"DETAIL_COUNT\", \"ITEM_BARCODE\", \"ETC\", \"USE_YN\", \"INST_USER\", \"INST_TIME\", \n#     \"UPD_USER\", \"UPD_TIME\", \"ITEM_TRANS\", \"ITEM_SHOOT\", \"LABEL_FIN\", \"STUDY\", \n#     \"TRAIN_CD\", \"ITEM_REFL_FIN\", \"SIMILAR\", \"LABEL_CNT_NEW\", \"LABEL_CNT_ALL\", \n#     \"BUNDLE_CODE\"\n# })\n\n# # 매 행(row)을 처리하면서 컬럼 비교 (최대 5개 행만 처리)\n# for idx, row in enumerate(collected_df.collect()):\n#     if idx >= 5:  # 5개까지만 처리\n#         break\n\n#     temp_df = row[\"$json$data_infer_schema$.temporary$\"]  # 열 안에 있는 JSON 문자열 가져오기\n#     # print(f\"Processing row {idx + 1}:\\n{temp_df}\")\n\n#     if temp_df is not None:\n#         # JSON 문자열을 파싱하여 딕셔너리로 변환\n#         json_data = json.loads(temp_df)\n#         if 'data' not in json_data.keys():\n#             print(\"skip: \",json_data.keys())\n#             continue\n#         cdc_columns = set(json_data['data'].keys())\n#         print(f\"Columns in JSON data: {cdc_columns}\")\n\n#         # Iceberg 컬럼과 비교하여 누락된 컬럼 찾기\n#         missing_columns = iceberg_columns - cdc_columns\n#         if missing_columns:\n#             print(f\"Schema mismatch: missing columns in stream data - {missing_columns}\")\n\n#             # 누락된 컬럼을 null 값으로 추가\n#             for col_name in missing_columns:\n#                 if col_name not in collected_df[\"$json$data_infer_schema$.temporary$\"]['data'].columns:\n#                     # 스키마에 없는 컬럼을 StringType으로 추가\n#                     _collected_df = collected_df[\"$json$data_infer_schema$.temporary$\"]['data'].withColumn(col_name, lit(None).cast(StringType()))\n\n#             print(f\"step 3: Added missing columns: {missing_columns}\")\n            \n#         # 결과 출력\n#         _collected_df.show(5, truncate=False)\n",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "AnalysisException: Cannot resolve column name \"$json$data_infer_schema$.temporary$\" among ($json$data_infer_schema$.temporary$); did you mean to quote the `$json$data_infer_schema$.temporary$` column?\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# collected_df[\"`$json$data_infer_schema$.temporary$`\"]",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "Column<'$json$data_infer_schema$.temporary$'>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# from pyspark.sql import SparkSession\n# import time\n# # Spark 세션 가져오기\n# # spark = SparkSession.builder.getOrCreate()\n\n# # 메모리 테이블에서 데이터를 읽어오는 함수\n# def fetch_stream_data():\n#     return spark.sql(\"SELECT * FROM stream_data limit 10\")\n\n# # 테스트를 위한 배치 처리 함수 호출\n# def test_process_batch():\n#     print(\"a\")\n#     for _ in range(10):  # 무한 루프 대신 제한된 반복으로 테스트\n#         # 메모리에서 데이터 가져오기\n#         print(\"b\")\n#         stream_data_df = fetch_stream_data()\n#         print(stream_data_df)\n\n#         if stream_data_df.count() > 0:\n#             # 데이터가 있을 때만 처리\n#             processBatch(stream_data_df, batch_id=\"test_batch\")\n#         else:\n#             print(\"No new data in the stream_data table.\")\n\n#         # 적절한 대기 시간을 두어 불필요한 로드를 방지 (예: 10초마다 체크)\n        \n#         print(\"c\")\n#         time.sleep(10)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "processBatch(collected_df,1)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "start\nError in step 1: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table mst_item_iceberg. StorageDescriptor#InputFormat cannot be null for table: mst_item_iceberg (Service: null; Status Code: 0; Error Code: null; Request ID: null; Proxy: null)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# from pyspark.sql import functions as F\n# kds_df = glueContext.create_data_frame.from_options(\n#     connection_type=\"kinesis\",\n#     connection_options={\n#         \"typeOfData\": \"kinesis\",\n#         \"streamARN\": \"arn:aws:kinesis:ap-northeast-2:646298132551:stream/cdc-test\",\n#         \"classification\": \"json\",\n#         \"startingPosition\": \"LATEST\",\n#         \"inferSchema\": \"true\",\n#     },\n#     transformation_ctx=\"kds_df\",\n#     additional_options={\"samplingRatio\": 0.15}\n# )\n\n# # 변환된 스키마 확인\n# checkpointPath = f\"{args['TempDir']}/{args['JOB_NAME']}/checkpoint/\"\n# # kds_df.printSchema()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "Py4JJavaError: An error occurred while calling o177.getDynamicFrame.\n: java.lang.UnsupportedOperationException: Streaming data source doesn't support dynamic frame\n\tat com.amazonaws.services.glue.StreamingDataSource.getDynamicFrame(DataSource.scala:1171)\n\tat com.amazonaws.services.glue.StreamingDataSource.getDynamicFrame$(DataSource.scala:1171)\n\tat com.amazonaws.services.glue.KinesisDataSource.getDynamicFrame(DataSource.scala:1474)\n\tat com.amazonaws.services.glue.StreamingDataSource.getDynamicFrame(DataSource.scala:1172)\n\tat com.amazonaws.services.glue.StreamingDataSource.getDynamicFrame$(DataSource.scala:1172)\n\tat com.amazonaws.services.glue.KinesisDataSource.getDynamicFrame(DataSource.scala:1474)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "job.commit()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "GlueArgumentError: the following arguments are required: --JOB_NAME\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# # 스키마 추론 버전\n# # kds_df = glueContext.create_data_frame.from_options(\n# #     connection_type=\"kinesis\",\n# #     connection_options={\n# #         \"typeOfData\": \"kinesis\",\n# #         \"streamARN\" : \"arn:aws:kinesis:ap-northeast-2:646298132551:stream/cdc-test\",\n# #         \"classification\": \"json\",\n# #         \"startingPosition\": \"TRIM_HORIZON\",\n# #         \"inferSchema\": \"true\",\n# #     },\n# #     transformation_ctx=\"kds_df\",\n# # )\n\n# # 스키마 명시 버전 \n# from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, DoubleType\n\n# schema = StructType([\n#     StructField(\"data\", StructType([\n#         StructField(\"SEQ\", IntegerType(), True),\n#         StructField(\"CMP_CD\", StringType(), True),\n#         StructField(\"BRAND_CD\", StringType(), True),\n#         StructField(\"STORE_TP\", IntegerType(), True),\n#         StructField(\"ITEM_CD\", StringType(), True),\n#         StructField(\"ITEM_SEQ\", IntegerType(), True),\n#         StructField(\"AIRPORT_DIV\", IntegerType(), True),\n#         StructField(\"ITEM_NM\", StringType(), True),\n#         StructField(\"IMAGE_PATH\", StringType(), True),\n#         StructField(\"DETAIL_COUNT\", IntegerType(), True),\n#         StructField(\"ITEM_BARCODE\", StringType(), True),\n#         StructField(\"ETC\", StringType(), True),\n#         StructField(\"USE_YN\", StringType(), True),\n#         StructField(\"INST_USER\", StringType(), True),\n#         StructField(\"UPD_USER\", StringType(), True),\n#         StructField(\"UPD_TIME\", StringType(), True),\n#         StructField(\"TRAIN_CD\", StringType(), True)\n#     ]), True),\n#     StructField(\"metadata\", StructType([\n#         StructField(\"timestamp\", StringType(), True),\n#         StructField(\"record-type\", StringType(), True),\n#         StructField(\"operation\", StringType(), True),\n#         StructField(\"partition-key-type\", StringType(), True),\n#         StructField(\"schema-name\", StringType(), True),\n#         StructField(\"table-name\", StringType(), True)\n#     ]), True)\n# ])\n\n# kds_df = glueContext.create_data_frame.from_options(\n#     connection_type=\"kinesis\",\n#     connection_options={\n#         \"typeOfData\": \"kinesis\",\n#         \"streamARN\": \"arn:aws:kinesis:ap-northeast-2:646298132551:stream/cdc-test\",\n#         \"classification\": \"json\",\n#         \"startingPosition\": \"TRIM_HORIZON\",\n#         \"inferSchema\": \"false\",\n#     },\n#     format_options={\"schema\": schema},\n#     transformation_ctx=\"kds_df\",\n# )\n\n# # LATEST\n",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		}
	]
}